{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robot Navigation Statistics\n",
    "\n",
    "This Jupyter notebook contains code which reads in and explores the dataset we discussed in the seminar: navigation data from a mobile robot navigating in an office environment. You should read through the notebook and run each cell in turn. Try to best to understand the code as you run it, since your tasks will require you to alter and resuse it. \n",
    "\n",
    "Before going further we'll import some common maths and plotting libraries. If this step fails, please refer to the supporting material you were previously sent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will work with a comma-separated values (csv) file of data. You should be able to find this file at [../data/tsc_y3_nav_stats.csv](../data/tsc_y3_nav_stats.csv). You should be able to navigate to it in the file browser on the right and open it. Do that first to get an idea of what's in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to read the data from the csv file into Python. The code below does this for you. It uses the built-in csv functionality in Python, combined with file reading. When the file is read in all the values are read in as strings. To allow us to work with the data, we convert the data row-by-row from strings into the native types. The `convert_row` method is specific to the data in this exercise. If you wish to use this approach for subsequent exercises you can copy this code and change the `convert_row` as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_records(filename):\n",
    "    \"\"\"\n",
    "    Read a CSV file into a list of dictionaries, where each \n",
    "    dictionary has keys taken from the column names in the file.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            convert_row(row)            \n",
    "            records.append(row)\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "def convert_row(row):\n",
    "    \"\"\"\n",
    "    Take a row and convert the values to the known data types.\n",
    "    \"\"\"\n",
    "    # duration is in seconds\n",
    "    row['operation_time'] = float(row['operation_time'])\n",
    "    row['time_to_waypoint'] = float(row['time_to_waypoint'])\n",
    "    # dates are converted to datetime objects\n",
    "    row['date_started'] = datetime.strptime(row['date_started'], '%Y-%m-%d %H:%M:%S')\n",
    "    row['date_at_node'] = datetime.strptime(row['date_at_node'], '%Y-%m-%d %H:%M:%S')\n",
    "    row['date_finished'] = datetime.strptime(row['date_finished'], '%Y-%m-%d %H:%M:%S')\n",
    "    # status of result is converted from a string/integer into a boolean \n",
    "    row['status_bin'] = (row['status_bin'] == '1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the above code to read in the navigation data from the csv file supplied, creating a list of records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_records('../data/tsc_y3_nav_stats.csv')\n",
    "print('Read in {} records'.format(len(data)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can take a look at the first entry in the data set. We do this by printing the first element of the list `data` which contains all the data we read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us the general structure of the data. Each data point is a sample of an attempt to navigate an edge in the robot's topological map. This is indicated by the `edge_id`. The navigation attempt starts from the `origin` node with the aim of reaching `target`. It may successfully reach that node, or it may end up somewhere else. The place it ends up is `final_node`. Each node is a 2D point plus an *influence zone* which is an area around that point. The `operation_time` field indicates the duration (in seconds) of the navigation action until the influence zone is reached. The `time_to_waypoint` is the time spent in the navigation action after the influence zone is reached until the action is terminated. Termination conditions may be the robot reaching the 2D point for the node, being asked to perform a new action which supersedes the current one, or failing (in which case the action is cancelled by another process due to a timeout or another failure condition). These durations can be matched to the dates as follows, `operation_time = date_finished - date_started`, `time_to_waypoint = date_finished - date_at_node` but they were  recorded with sub-second accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview\n",
    "\n",
    "When working with data, it is important to start by getting a general sense of the nature of that data. We can start by asking how many times each edge was traversed. \n",
    "\n",
    "To do this, we will first store the data in a dictionary where they keys are edge ids and the values are lists of data points from the associated edge id. The code below can achieve this grouping for any field in the dataset (i.e. it could do the same for `status_bin` instead of `edge_id`). \n",
    "\n",
    "**NOTE**: The code below makes quite heavy use of *dictionaries*. If you do not understand the behaviour of dictionaries in Python, refresh your memory with one of the suggested resources, e.g. https://realpython.com/python-dicts/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_data_by_field(datalist, field):\n",
    "    \"\"\"\n",
    "    Given a list of dictionaries, return a dictionary of lists where each sublist contains all dictionaries with the same value of that field, \n",
    "    and the key of that entry is that value.\n",
    "    \"\"\"\n",
    "    output = dict()\n",
    "    for d in datalist:\n",
    "        # get the value of the field for this entry\n",
    "        field_value = d[field]\n",
    "        # if this value has not been seen before, create a new output entry for it\n",
    "        if field_value not in output:\n",
    "            output[field_value] = []\n",
    "        # append the entry to the list for this field value\n",
    "        output[field_value].append(d)\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use this function to group the data by `edge_id`, so the result will be a mapping from each `edge_id` to a list of the data entries for that id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_groups = group_data_by_field(data, 'edge_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check we're happy with the result by looking at data for some specific edge ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_groups['WayPoint46_WayPoint39']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you should see a list with a single entry, and the single entry is a dictionary (i.e. a mapping of keys to values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_groups['SuppPrintCopy_SuppLocker6']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above this you should see a list with three entries, e.g. of the entires is a separate dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now count now many nav stats (i.e. data points) we have for each edge. We'll do this by creating a new dictionary that maps from edge ids to counts. Before looking at the code below feel free to attempt this yourself, or at least consider how you would need to implement it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dictionary\n",
    "edge_counts = dict()\n",
    "# for each edge_id and list of dictionaries in our data set\n",
    "for edge_id, stat_list in edge_groups.items():\n",
    "    # map the edge_id to the number of items in the stat list\n",
    "    edge_counts[edge_id] = len(stat_list)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write the same code in one line using a *dictionary comprehension* as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_counts = {edge_id: len(stat_list) for edge_id, stat_list in edge_groups.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dictionary `edge_counts` allows us to query the data for the number of data points per edge, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_id = 'SuppPrintCopy_SuppLocker6'\n",
    "print(f\"Edge {edge_id} has {edge_counts[edge_id]} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More usefully, we can visualise the count data as a histogram. To do this we'll need matched lists of edge IDs and counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_ids = list(edge_counts.keys())\n",
    "counts  = list(edge_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(edge_ids, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a bit ugly. To make it easier to extract information from, let's start by sorting the data. First we produce a list of (edge_id, count) tuples, then we sort that list. We can verify that this has worked as expected by printing the first and lest element of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of (edge_id, count) tuples\n",
    "edge_count_list = [(edge_id, len(stat_list)) for edge_id, stat_list in edge_groups.items()]\n",
    "\n",
    "# sort the list - `key=lambda x:x[1]` says to sort using the second item of each tuple for comparison.\n",
    "edge_count_list.sort(key=lambda x:x[1], reverse=True)\n",
    "\n",
    "# print out the first and last place items from the sorted list\n",
    "print(edge_count_list[0], edge_count_list[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the histogram we need matched lists of edge_ids and counts. To create this we extract the first element of every tuple into a list of edge ids, and a the second elements into the count list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_ids = []\n",
    "counts = []\n",
    "for tupl in edge_count_list:\n",
    "    edge_ids.append(tupl[0])\n",
    "    counts.append(tupl[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now produce a histogram chart that looks a bit cleaner. We'll also remove the labels from the x axis, which are impossible to read anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(edge_ids, counts)\n",
    "plt.xticks([])\n",
    "plt.ylabel('Count')\n",
    "plt.title('Edge data counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are clearly some interesting edges which have significantly more visits than the the others. Let's look at the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 10 edge ids\n",
    "top_edge_ids = edge_ids[:10]\n",
    "# first 10 counts\n",
    "top_counts = counts[:10]\n",
    "plt.bar(top_edge_ids, top_counts)\n",
    "\n",
    "plt.ylabel('Count')\n",
    "plt.title('Top 10 edge data counts')\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Task A: Plotting\n",
    "\n",
    "Make sure you follow what the code above is doing. Once you are happy with the provided code, write your own code to complete as many of the following tasks as you want. You may use external libraries (e.g. numpy, scipy) if you wish. Although you can use the empty cells below this one to contain your solutions, you may find it easier to structure your work if you create a new notebook and copy across the the relevant parts of this one.\n",
    "\n",
    "1. Print out the edges with the maximum and minimum number of visits.\n",
    "1. Plot a histogram with the 20 fewest datapoints.\n",
    "1. Determine the mean, median, and mode number of nav stats per edge.\n",
    "1. Adapt the histograph above so that each bar has a green part for the count of successful navigation actions, and a red part for a count of the unsuccessful ones. You can use the `status_bin` field for this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task A.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A.4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Estimation for Edge Actions\n",
    "\n",
    "When we are using this navigation data to build a Markov decision process to plan robot navigation routes, we are interested in two properties of this data: the duration of the navigation action, and whether the action is likely to succeed or not. We can now move on to looking at these features of each edge. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with we'll build a list of (operation_time, date_started, operation_succeeded) tuples for every action on each edge. This will allow us to consider that data together more easily later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_operation_data = dict()\n",
    "\n",
    "for edge_id, rows in edge_groups.items():    \n",
    "    edge_operation_data[edge_id] = [(r['operation_time'], r['date_started'], r['status_bin']) for r in rows]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's choose an edge to focus on for some analysis. Focussing on one part of your dataset can allow you get a better handle on your dataset, but be careful not to make unwarranted generalisations to other parts. In this case we're going to focus on one of the edges with lots of data, 'SuppLockOutside_KitchenEntrance'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_edge = 'SuppLockOutside_KitchenEntrance'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an initial feel for the operation duration data on this edge let's start by looking at few entries, so we understand what we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_operation_data[focus_edge][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you should see tuples of operation time, date, and status for the first 10 entries for the edge we decided to focus on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B: Success Probability\n",
    "\n",
    "\n",
    "1. Complete the function `estimate_success_probability` to estimate the success probability for a single edge. The input should be a list of data tuples for that edge, as printed out above. You should treat the robot attempting to the drive on the edge as a Bernoulli trial and use the parater estimation technique we saw in the first lecture. \n",
    "2. Implement code which uses `estimate_success_probability` to estimate the success probability for every edge in the dataset\n",
    "3. Use the reults to report the edges with the highest and lowest success probabilities. Note that multiple edges can have the same probability estimate.\n",
    "4. Plot a histogram of the success probabilities over the whole dataset.\n",
    "\n",
    "\n",
    "Below we provide empty cells to correspond to these steps, but you can structure your solution however you prefer (i.e. you can use these cells, add your own, or do it all in a single cell... although the latter is bad practice since it limits resuability).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B.1: Single Edge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_success_probability(edge_data): \n",
    "    # Replace this with your code\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The success probabilty for edge {focus_edge} is estimated to be {estimate_success_probability(edge_operation_data[focus_edge])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have implemented this correctly, you should see that the success probability for the chosen edge is 0.91 (to 2 sf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B.2: Estimate the success probability for every edge in the dataset\n",
    "\n",
    "To allow you to reuse the results in subsequent steps store the results as a list of (edge_id, success probability) tuples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_id_probs = []\n",
    "for edge_id, edge_data in edge_operation_data.items():\n",
    "    # edit the line below to store the requested data in each tuple\n",
    "    edge_tuple = ()\n",
    "    edge_id_probs.append(edge_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B.3: \n",
    "\n",
    "Report the edges with the highest and lowest success probabilities. Note that multiple edges can have the same probability estimate.\n",
    "\n",
    "If you need help in doing this, look back to the code above where we examined the number of data points per edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B.4: \n",
    "\n",
    "Plot a histogram of the success probabilities over the whole dataset.\n",
    "\n",
    "Does the data show you anything unexpected?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task C: Edge Duration\n",
    "\n",
    "Now we are going to look at the duration data (`operation_time`) per edge. \n",
    "\n",
    "We will assume that the duration of action per edge is normally distributed. This is not an ideal model in practice, but will suffice for this exercise. Consider why it may not be a good distribution for action durations.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task C.1: Single Edge Estimation\n",
    "\n",
    "Once again focussing on a single edge (`focus_edge`), write a function that estimates the mean and variance parameters of a normal distribution based on the durations of this edge. You can either implemented the formulae we proved in the first session or use a function from a library. If you use a library function, be careful to check the return values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_duration_parameters(edge_data):\n",
    "    # Replace this with your code\n",
    "    mu = 0\n",
    "    var = 1\n",
    "    return mu, var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'focus_edge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2r/32c_88251s5_lt_bfbr7c7gw0000gn/T/ipykernel_29148/357038387.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The duration parameters for edge {focus_edge} is estimated to be {estimate_duration_parameters(edge_operation_data[focus_edge])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'focus_edge' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"The duration parameters for edge {focus_edge} is estimated to be {estimate_duration_parameters(edge_operation_data[focus_edge])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If correct, your code should return at mean value of 18.09s (to 2 dp), and a variance of 142.43 (to 2 dp). That variance corresponds to standard deviation of roughly 12. Given the mean is around 18, this makes it look as if we might have some quite widely distributed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Edge PDF and Visualisation\n",
    "\n",
    "Lets have a look at the edge data more closely. The code below plots a histogram of durations. When we use the `density=True` argument we convert the histogram into a probability density function. This will allow us to later compare it to the likelihood function of a normal distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_durations = [edge[0] for edge in edge_operation_data[focus_edge]]\n",
    "plt.hist(edge_durations, density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task C.2: PDF of a Normal Distribution\n",
    "\n",
    "\n",
    "Let's see how that plot matches up to the normal distribution we've estimated. Do do this, we'll need to compute the *probability density value* (pdf) of a normal distribution for a given value of x. Complete the function below to do this, again using either the formula from the first session or a library function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_pdf(x, mu, var):\n",
    "    # Your code here\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that method we can plot the normal curve on top of the histogram. Note that if you haven't implemented `norm_pdf` yet you should only see a flat orange line for the pdf curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, var = estimate_duration_parameters(edge_operation_data[focus_edge])\n",
    "# create our x values\n",
    "x = np.linspace(np.min(edge_durations), np.max(edge_durations), 1000)\n",
    "# use our pdf function to compute a y value for each x\n",
    "y = [norm_pdf(x_i, mu, var) for x_i in x]\n",
    "# then plot both\n",
    "plt.hist(edge_durations, density=True)\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curve looks quite well matched to the histogram, but the the high variance is in part due to those high duration values on the right of the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Edge - IID\n",
    "\n",
    "For maximum likelihood estimation to return the correct result, the values in the dataset should be independent and identically distributed (the IID assumption). So far we have ignored that assumption. However, our dataset contains extra information we can use to test the assumption. In particular, whether or not the edge action succeeded is likely to play a significant role in the duration of the action. Let's explore that idea further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task C.3: Duration Lists \n",
    "\n",
    "In the cell below create two lists of duration values for the focus edge. One list should contain durations of edges actions which succeeded, the other list should contain durations from failures. Plot values from both lists on the same histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill these lists with the requested duration data\n",
    "edge_success_durations = []\n",
    "edge_fail_durations = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(edge_success_durations), len(edge_fail_durations))\n",
    "\n",
    "if len(edge_success_durations) > 0:\n",
    "    plt.hist(edge_success_durations, density=True)\n",
    "    \n",
    "if len(edge_fail_durations) > 0:\n",
    "    plt.hist(edge_fail_durations, density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check, you should have 172 duration values in your success list and 16 in your fail list.\n",
    "\n",
    "You should also see that the duration distribution of successful actions looks pretty different to the distribution of failed actions. \n",
    "\n",
    "Let's use this observation and fit separate normal distributions to each list and add those to the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with code to esimate success (*_s) and failure (*_f) parameters separately\n",
    "mu_s, var_s = 0, 1\n",
    "mu_f, var_f = 0, 1\n",
    "\n",
    "y_s = [norm_pdf(x_i, mu_s, var_s) for x_i in x]\n",
    "y_f = [norm_pdf(x_i, mu_f, var_f) for x_i in x]\n",
    "\n",
    "if len(edge_success_durations) > 0:\n",
    "    plt.hist(edge_success_durations, density=True)\n",
    "    \n",
    "if len(edge_fail_durations) > 0:\n",
    "    plt.hist(edge_fail_durations, density=True)\n",
    "    \n",
    "plt.plot(x, y_s)\n",
    "plt.plot(x, y_f)\n",
    "\n",
    "print(mu_s, var_s, mu_f, var_f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This certainly looks as if the duration distributions for success and failure are goverened by different distributions. In particular the variance of the failure duration distribution is a lot higher than for successes. This makes sense: successes should largely be predictable since they happen when the robot is free to navigate; failures can be due to many different causesm each of which may produce a different delay. In fact, it may be the case that the different failure cases should be modelled separately, but that's beyond the scope of this exericse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task D: Multiple Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use everything you have written above to determine:\n",
    "1. the edge with the lowest expectated duration for a successful execution\n",
    "2. the edge with the most uncertain duration distribution for failure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task D.1\n",
    "\n",
    "Implement the code to find the edge with the lowest expectated duration for a successful execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that KitchenPod2_KitchenActRec2 has the lowest expected success duration, around 5.09s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task D.2\n",
    "\n",
    "Implement the code to the edge with the most uncertain duration distribution for failure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that the  fail duration with the highest variance is Station1_ChargingPoint1 with 2470.17.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task E: Multiple Edges - Extension\n",
    "\n",
    "_This is an optional extension that you should only consider if you have spare time and have completed everything above._\n",
    "\n",
    "It is common practice in data science to filter out outliers from the data. For example, any value that falls a certain number of standard deviations from the mean could be considered an outlier. Repeat the single edge processing above, but remove outliers that fall more than 3 standard deviations away from the mean of the appropriate distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
